---
title: "Polynomial Regression Demo, Winter 2024"
author: "Zhiwei Tan, Katrin JÃµgi"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, tidy=TRUE)

library(caret)
library(MASS)
library(tidyverse)

```

# Introduction
When the relationship between the output variable and the predictor variable is not linear, nonlinear regression can be used to fit the data to a model. Types of nonlinear regression are polynomial regression, stepwise regression and splines. The current demo focuses on the polynomial regression. 

Polynomial regression offers flexibility in capturing non-linear relationships by increasing the polynomial order. This allows for a closer fit to the data, potentially revealing complex patterns. However, this flexibility also increases the risk of overfitting, where the model memorizes noise instead of learning generalizable trends.

# Context
For the demo, we are using the Boston housing data set from the MASS library. This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass.
The Boston data frame has 506 observations on 14 variables.

**dataset columns can be seen using help(Boston)**

- CRIM - per capita crime rate by town
- ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
- INDUS - proportion of non-retail business acres per town.
- CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
- NOX - nitric oxides concentration (parts per 10 million)
- RM - average number of rooms per dwelling
- AGE - proportion of owner-occupied units built prior to 1940
- DIS - weighted distances to five Boston employment centres
- RAD - index of accessibility to radial highways
- TAX - full-value property-tax rate per \$10,000
- PTRATIO - pupil-teacher ratio by town
- B - 1000(Bk - 0.63)\^2 where Bk is the proportion of blacks by town
- LSTAT - % lower status of the population
- MEDV - Median value of owner-occupied homes in \$1000's


For the purposes of the current demo, the outcome variable is MEDV - Median value of owner-occupied homes in \$1000's and the predictor variable is LSTAT - % lower status of the population. 

We set out to explore whether polynomial regression is appropriate when modeling the effect of % lower status of the population on the housing values.

##Load data and data processing

Load data Boston housing data from MASS package. Check for missing values and data types. Split the data for training and test.

```{r}
data('Boston') #load data

sum(is.na(Boston)) #check for missing values

str(Boston) #view format of the data

head(Boston) #view top six observations

```

In order to be able to test the generalizability of the selected model, we split our data set into the train and test set with 80/20 split.

```{r}

set.seed(1234)

training.samples <- Boston$medv %>% createDataPartition(p = 0.8, list = FALSE)

train.data  <- Boston[training.samples, ]

test.data <- Boston[-training.samples, ]

```


# Linear Regression

We want to compare a linear regression on a polynomial regression. First, we check the assumptions of the linear model and look for any outliers.

```{r}

Boston.reg <- lm(medv ~ lstat, data = train.data)

plot(Boston.reg)

plot(Boston.reg, 4) #Cook's distance

```
Polynomial regression model is sensitive to outliers, even one or two outliers can badly affect its performance. We do not observe any potentially influential points on the Residuals vs Leverage plot.

The Scale-Location plot does not display any specific shape, thus there is not enough evidence against the assumption of equality of residual variance across the range of the fitted values.

The Residuals vs Fitted plot suggests that the the assumption of linearity is not necessarily met across the range of the fitted values.

The Normal Q-Q plot indicates that, though not definitive, there is not enough evidence against the assumption of normality across the range of the fitted values.

#Summary and visual of the linear model
```{r}

summary(Boston.reg)

ggplot(train.data, aes(x = lstat, y = medv)) +  geom_point(alpha = 0.2) +  geom_smooth(method = "lm", se = FALSE) + labs(title = "Linear regression", x = "% lower status population", y = "median home values")

```

R-squared for the linear model is 0.5485, which means 54.85% of the variance in the housing prices (outcome variable) is explained by the % of lower status population (predictor variable)

It can be observed visually that the observations do not spread uniformly around the linear line of best fit. Curvature in the data can be observed, suggesting that a polynomial regression line could better explain the data.


# Polynomial regression

Polynomial regression is a type of linear regression that can capture nonlinear relationships between predictor and outcome variables. It achieves this by fitting smooth curves (e.g., quadratic, cubic) to the data using least squares. Similar to linear regression, R-squared helps evaluate the model's fit, with higher values indicating better performance.

## Quadratic and Cubic regression

Lets visualize the relationship between the outcome and the predictor variable with 2nd degree (quadratic) and 3rd degree (cubic) polynomial fits to see if we identify potential non-linearity. The observation values are set to 20% opacity so we can see the spread, and linear and polynomial lines have been overlapped.

```{r}

ggplot(train.data, aes(x = lstat, y = medv)) + geom_point(alpha = 0.2) + geom_smooth(method = "lm", se = FALSE, size = 0.5) + geom_smooth(method = "lm", formula = y ~ poly(x, 2), se = FALSE, color = "red") + labs(title = "Quadratic (2nd degree polynomial) regression", x = "% lower status population", y = "median home values")

ggplot(train.data, aes(x = lstat, y = medv)) +  geom_point(alpha = 0.2) + geom_smooth(method = "lm", se = FALSE, size = 0.5) + geom_smooth(method = "lm", formula = y ~ poly(x, 3), se = FALSE, color = "green") + labs(title = "Cubic (3rd degree polynomial) regression", x = "% lower status population", y = "median home values")

```
Based on visual inspection, the polynomial regression lines seem to follow the data more closely, but it is not obvious which is performing better of the two.

## Polynomial code 

Although polynomial regression allows for a nonlinear relationship between predictor and outcome variables, it is still considered linear regression since it is linear in the regression coefficients. Thus, standard linear regression software can be used. The linear model is extended by adding extra predictors, obtained by raising the original predictors to a power. Since pure U-shaped or S-shaped relationships are rare, polynomials are usually specified in addition to a linear term in order to pick up deviations from a linear relationship. 

Why not simply add lstat^2 directly? Standard linear regression software doesn't recognize terms like lstat^2 on their own.

```{r}

Boston.poly.wrong.1 <- lm(medv ~ lstat + lstat^2, data=train.data)

summary(Boston.poly.wrong.1)

```
Polynomial regression requires transforming the predictor variable into its polynomial form.

- poly(x, degree, raw = TRUE): This function takes the predictor x, the desired polynomial degree (e.g., 2 for squared), and sets raw = TRUE to ensure the coefficients align with the I() function coefficients.

- I(x^power): This function takes the predictor x raised to the desired power.

Note: While raw = TRUE in poly() might lead to slightly different coefficients compared to without it, interpreting polynomial coefficients can be challenging anyway. Both methods ultimately fit the same curve.

```{r}

Boston.poly.2.P <- lm(medv ~ lstat + poly(lstat, 2, raw = TRUE), data=train.data) #poly()

summary(Boston.poly.2.P)

Boston.poly.2.I <- lm(medv ~ lstat + I(lstat^2), data=train.data) #I()

summary(Boston.poly.2.I)

```

#Comparison of the linear and quadratic model
Coefficients in polynomial regression represent the change in the outcome variable for a one-unit increase in the corresponding polynomial term of the predictor variable. While interpreting these coefficients can be complex, the overall R-squared increase (e.g., from 54.85% to 65.63%) suggests the polynomial model is a better fit in this case.

#Higher order polynomial models
Since the quadratic model was an improvement over the linear model, we decided to explore higher order polynomial models, to see if we can further improve our model.

The I() function requires including all polynomial terms up to the desired power. For example, including only I(lstat^3) misses the essential second-order term.

```{r}

Boston.poly.wrong.2 <- lm(medv ~ lstat + I(lstat^3), data=train.data)

summary(Boston.poly.wrong.2)

```

```{r}
Boston.poly.3 <- lm(medv ~ lstat + I(lstat^2) + I(lstat^3), data=train.data)

summary(Boston.poly.3)

Boston.poly.4 <- lm(medv ~ poly(lstat, 4, raw = TRUE), data=train.data)

summary(Boston.poly.4)

Boston.poly.5 <- lm(medv ~ poly(lstat, 5, raw = TRUE), data=train.data)

summary(Boston.poly.5)

Boston.poly.6 <- lm(medv ~ poly(lstat, 6, raw = TRUE), data=train.data)

summary(Boston.poly.6)

```

```{r}

par(mfrow = c(1,3))
ggplot(train.data, aes(x = lstat, y = medv)) + geom_point(alpha = 0.2) + geom_smooth(method = "lm", se = FALSE, size = 0.5) + geom_smooth(method = "lm", formula = y ~ poly(x, 4), se = FALSE, color = "black") + labs(title = "4th degree polynomial regression", x = "% lower status population", y = "median home values")

ggplot(train.data, aes(x = lstat, y = medv)) +  geom_point(alpha = 0.2) + geom_smooth(method = "lm", se = FALSE, size = 0.5) + geom_smooth(method = "lm", formula = y ~ poly(x, 5), se = FALSE, color = "purple") + labs(title = "5th degree polynomial regression", x = "% lower status population", y = "median home values")

ggplot(train.data, aes(x = lstat, y = medv)) +  geom_point(alpha = 0.2) + geom_smooth(method = "lm", se = FALSE, size = 0.5) + geom_smooth(method = "lm", formula = y ~ poly(x, 6), se = FALSE, color = "blue") + labs(title = "6th degree polynomial regression", x = "% lower status population", y = "median home values")

```

# Comparison of the R-squared

```{r}

summary(Boston.reg)$r.squared #linear
summary(Boston.poly.2.P)$r.squared #quadratic
summary(Boston.poly.3)$r.squared #cubic
summary(Boston.poly.4)$r.squared #quartic
summary(Boston.poly.5)$r.squared #quintic
summary(Boston.poly.6)$r.squared #sextic

```

R-squared values suggest higher-order polynomial models explain more variance in the outcome variable (increasing R-squared), 71.19% for the 6th order polynomial compared to 54.85% for the linear model. However, visually, the differences are subtle.


## Model Comparison

To move beyond visual inspection, we employed formal techniques:

- F-tests (anova): These tests helped compare models and identify diminishing improvements with higher polynomial degrees.

- AIC: This metric considers both model fit and complexity. A lower AIC value indicates a better balance between explaining the data and avoiding overfitting.

```{r}

anova(Boston.reg, Boston.poly.2.P, Boston.poly.3, Boston.poly.4, Boston.poly.5, Boston.poly.6) #F-test

aic <- c(AIC(Boston.reg), AIC(Boston.poly.2.P), AIC(Boston.poly.3), AIC(Boston.poly.4), AIC(Boston.poly.5), AIC(Boston.poly.6))
df <- data.frame(aic) #AIC

plot(aic, data = aic, main = "Akaike Information Criterion (AIC)", xlab = "Order of the polynomial model", ylab = "AIC")

```

The hypothesis that is checked at each step is that the decrease in RSS is not significant. The hypothesis is rejected if the p-value is smaller than a given significance level (say 0.05). 

The F-test and AIC comparison suggests that the higher order models are increasingly better fit, compared to the linear or lower order models, up to and including the 5th order polynomial.


## Over-Fitting / Generalization Limitations

#Test model on test data
While the F-tests and AIC suggest a 5th order model might fit best, relying solely on these metrics can be misleading. To validate the model's generalizability, we used the trained model on our test data. This ensures the chosen model performs well on unseen data, not just the training data used to fit it.

We compared the Root Mean Squared Errors (RMSE) and the adjusted R-squared (R2) values of the trained model on the test data.

```{r}
print('--- Linear ---')

predictions.l <- Boston.reg %>% predict(test.data)

RMSE(predictions.l, test.data$medv)

R2(predictions.l, test.data$medv)

print('--- 2nd order ---')

predictions.2 <- Boston.poly.2.P %>% predict(test.data)

RMSE(predictions.2, test.data$medv)

R2(predictions.2, test.data$medv)

print('--- 3rd order ---')

predictions.3 <- Boston.poly.3 %>% predict(test.data)

RMSE(predictions.3, test.data$medv)

R2(predictions.3, test.data$medv)

print('--- 4th order ---')

predictions.4 <- Boston.poly.4 %>% predict(test.data)

RMSE(predictions.4, test.data$medv)

R2(predictions.4, test.data$medv)

print('--- 5th order ---')

predictions.5 <- Boston.poly.5 %>% predict(test.data)

RMSE(predictions.5, test.data$medv)

R2(predictions.5, test.data$medv)

print('--- 6th order ---')

predictions.6 <- Boston.poly.6 %>% predict(test.data)

RMSE(predictions.6, test.data$medv)

R2(predictions.6, test.data$medv)

```

```{r}

rmse <- c(RMSE(predictions.l, test.data$medv), RMSE(predictions.2, test.data$medv), RMSE(predictions.3, test.data$medv), RMSE(predictions.4, test.data$medv), RMSE(predictions.5, test.data$medv), RMSE(predictions.6, test.data$medv))
r2 <- c(R2(predictions.l, test.data$medv), R2(predictions.2, test.data$medv), R2(predictions.3, test.data$medv), R2(predictions.4, test.data$medv), R2(predictions.5, test.data$medv), R2(predictions.6, test.data$medv))
df1 <- data.frame(rmse)
df2 <- data.frame(r2)
plot(rmse, data = df1, main = "Root Mean Squared Errors (RMSE)", xlab = "Order of the polynomial model", ylab = "RMSE")
plot(r2, data = df2, main = "Adjusted R-squared", xlab = "Order of the polynomial model", ylab = "Adjusted R-squared")

```

Our evaluation confirms that polynomial models are an improvement over the linear model, indicated by higher adjusted R-squared and lower RMSE values. However, this improvement plateaus and even reverses at some point. This is a telltale sign of overfitting, where the model memorizes noise in the training data and loses its ability to generalize to unseen data. Thus, the simpler models perform better on new data.

#Conclusion
In conclusion, fitting higher order polynomial terms to the data suggested that the 5th order polynomial model might explain the data the best. After testing the model on the test data, we conclude that a polynomial model is an improvement over a linear model, but since higher order polynomial models exhibit signs of overfitting, we select the quadratic model.

#Reading list
1: http://ndl.ethernet.edu.et/bitstream/123456789/26799/1/122.Henning%20Best%2C%20Christof%20Wolf-.pdf

2: https://online.stat.psu.edu/stat501/lesson/9/9.8

3: https://www.statology.org/polynomial-regression/

4: https://home.iitk.ac.in/~shalab/regression/Chapter12-Regression-PolynomialRegression.pdf

5: https://towardsdatascience.com/polynomial-regression-bbe8b9d97491

6: https://statisticsbyjim.com/regression/choose-linear-nonlinear-regression/

7: https://www.analyticsvidhya.com/blog/2021/07/all-you-need-to-know-about-polynomial-regression/

8: (R reference): https://bookdown.org/ssjackson300/Machine-Learning-Lecture-Notes/polynomial-regression.html


9: (R reference): https://www.statology.org/polynomial-regression-r/


10: (R reference): https://medium.com/analytics-vidhya/machine-learning-project-3-predict-salary-using-polynomial-regression-7024c7bace4f